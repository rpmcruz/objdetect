{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b266a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install git+https://github.com/rpmcruz/objdetect.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9689c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import objdetect as od"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca13c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed62e03",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "Let's use [Albumentations](https://albumentations.ai/) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "    A.Resize(int(256*1.1), int(256*1.1)),\n",
    "    A.RandomCrop(256, 256),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(0.1, 0.1, p=1),\n",
    "    A.Normalize(),\n",
    "    ToTensorV2()\n",
    "], bbox_params=A.BboxParams(format='albumentations', label_fields=['classes']))\n",
    "\n",
    "inv_normalize = torchvision.transforms.Normalize((-0.485/0.229, -0.456/0.224, -0.406/0.225), (1/0.229, 1/0.224, 1/0.225))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473ccc2",
   "metadata": {},
   "source": [
    "## Dataset loader\n",
    "\n",
    "Here, we will sub-class the data load code that comes with TorchVision. Something to keep in mind is the format and units of your bounding boxes. <u>We recommend using the 0-1 normalized x1y1x2y2 format.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f1f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyVOCDetection(torchvision.datasets.VOCDetection):\n",
    "    labels = ['person', 'bird', 'cat', 'cow', 'dog', 'horse', 'sheep', 'aeroplane', 'bicycle', 'boat', 'bus', 'car', 'motorbike', 'train', 'bottle', 'chair', 'diningtable', 'pottedplant', 'sofa', 'tvmonitor']\n",
    "\n",
    "    def __init__(self, root, split, dict_transform=None, download=False):\n",
    "        super().__init__(root, image_set=split, download=download)\n",
    "        self.dict_transform = dict_transform\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        image, xml = super().__getitem__(i)\n",
    "        w, h = image.size\n",
    "        objs = xml['annotation']['object']\n",
    "        bboxes = torch.tensor([(\n",
    "            float(o['bndbox']['xmin']) / w,\n",
    "            float(o['bndbox']['ymin']) / h,\n",
    "            float(o['bndbox']['xmax']) / w,\n",
    "            float(o['bndbox']['ymax']) / h,\n",
    "        ) for o in objs])\n",
    "        classes = torch.tensor([self.labels.index(o['name']) for o in objs])\n",
    "        d = {'image': np.array(image), 'bboxes': bboxes, 'classes': classes}\n",
    "        if self.dict_transform:\n",
    "            d = self.dict_transform(**d)\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = MyVOCDetection('data', 'train', transform, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ad3fe",
   "metadata": {},
   "source": [
    "Let's look at the first sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ds[0]\n",
    "print(d.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.utils.plot(inv_normalize(d['image']), d['bboxes'], [ds.labels[k] for k in d['classes']], grid=(8, 8))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8948e2a",
   "metadata": {},
   "source": [
    "Naturally, the number of bounding boxes varies for each image, therefore they cannot be turned into tensors, so we need to specify a `collate` function for how the batches should be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = torch.utils.data.DataLoader(ds, 16, True, collate_fn=od.utils.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104b610",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d6976",
   "metadata": {},
   "source": [
    "We will prepare a one-stage model where for each location in the grid predicts: if there is an object (score), and if so, what is the object class and bounding box. Like the object detection models that come with torchvision (see e.g. [FCOS](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fcos_resnet50_fpn.html#torchvision.models.detection.fcos_resnet50_fpn)), the behavior changes if in `train` or `eval` mode, but we don't do exactly what they do. In `train` mode, we return the *unprocessed* scores/classes/bboxes grids. In `eval` mode, we return the *processed* classes/bboxes in the form of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50511d",
   "metadata": {},
   "source": [
    "![](model.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3262d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        resnet = torchvision.models.resnet50(weights='DEFAULT')\n",
    "        self.backbone = torch.nn.Sequential(*list(resnet.children())[:-2])\n",
    "        self.scores = torch.nn.Conv2d(2048, 1, 1)\n",
    "        self.classes = torch.nn.Conv2d(2048, 20, 1)\n",
    "        self.bboxes = torch.nn.Conv2d(2048, 4, 1)\n",
    "\n",
    "    def forward(self, x, threshold=0.5):\n",
    "        x = self.backbone(x)\n",
    "        scores = self.scores(x)\n",
    "        classes = self.classes(x)\n",
    "        bboxes = self.bboxes(x)\n",
    "        if not self.training:\n",
    "            # when in evaluation mode, convert the output grid into a list of bboxes/classes\n",
    "            scores = torch.sigmoid(scores)\n",
    "            hasobjs = scores >= threshold\n",
    "            scores = od.grid.inv_scores(hasobjs, scores)\n",
    "            bboxes = od.grid.inv_offset_logsize_bboxes(hasobjs, bboxes)\n",
    "            classes = od.grid.inv_classes(hasobjs, classes)\n",
    "            bboxes, classes = od.post.NMS(scores, bboxes, classes)\n",
    "            return bboxes, classes\n",
    "        return scores, bboxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b6817",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel().to(device)\n",
    "scores_loss = torch.nn.BCEWithLogitsLoss()\n",
    "bboxes_loss = torch.nn.MSELoss(reduction='none')\n",
    "classes_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    for imgs, targets in tqdm(tr):\n",
    "        imgs = imgs.to(device)\n",
    "        preds_scores, preds_bboxes, preds_classes = model(imgs)\n",
    "\n",
    "        slices = od.grid.slices_center_locations(8, 8, targets['bboxes'])\n",
    "        scores = od.grid.scores(8, 8, slices, device=device)\n",
    "        bboxes = od.grid.offset_logsize_bboxes(8, 8, slices, targets['bboxes'], device=device)\n",
    "        classes = od.grid.classes(8, 8, slices, targets['classes'], device=device)\n",
    "\n",
    "        loss_value = \\\n",
    "            scores_loss(preds_scores, scores) + \\\n",
    "            (scores * bboxes_loss(preds_bboxes, bboxes)).mean() + \\\n",
    "            (scores * classes_loss(preds_classes, classes)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += float(loss_value) / len(tr)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Avg loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fe0f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "imgs = torch.stack([ds[i]['image'] for i in range(12)])\n",
    "bboxes, classes = model(imgs.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84682ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    plt.subplot(3, 4, i+1)\n",
    "    od.utils.plot(inv_normalize(imgs[i]), bboxes[i].detach().cpu(), [int(k) for k in classes[i]])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
