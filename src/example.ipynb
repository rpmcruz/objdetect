{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import objdetect as od\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ca13c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed62e03",
   "metadata": {},
   "source": [
    "## Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31c1ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformations = od.aug.Compose([\n",
    "    od.aug.Resize(int(256*1.1), int(256*1.1)),\n",
    "    od.aug.RandomCrop(256, 256),\n",
    "    od.aug.RandomHflip(),\n",
    "    od.aug.RandomBrightnessContrast(0.1, 0.1),\n",
    "    od.aug.Normalize(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f473ccc2",
   "metadata": {},
   "source": [
    "## Dataset loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7938fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = od.data.VOCDetection('data', 'train', transformations, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300ad3fe",
   "metadata": {},
   "source": [
    "Let's look at the first sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4d42f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = ds[0]\n",
    "print(d.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f1db95",
   "metadata": {},
   "source": [
    "Each sample is composed by an 'image', and respective objects represented by their 'bboxes' and 'classes'. The 'bboxes' are in the format x1y1x2y2 and are 0-1 normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1c25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "od.plot.image(d['image'])\n",
    "od.plot.grid_lines(d['image'], 8, 8)\n",
    "od.plot.bboxes(d['image'], d['bboxes'])\n",
    "od.plot.classes(d['image'], d['bboxes'], d['classes'], ds.labels)\n",
    "od.plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8948e2a",
   "metadata": {},
   "source": [
    "Naturally, the number of bounding boxes varies for each image, therefore they cannot be turned into tensors, so we need to specify a `collate` function for how the batches should be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d1240",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = torch.utils.data.DataLoader(ds, 16, True, collate_fn=od.data.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3104b610",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2d6976",
   "metadata": {},
   "source": [
    "We will prepare a one-stage model where for each location in the grid predicts: if there is an object (score), and if so, what is the object class and bounding box. Like the object detection models that come with torchvision (see e.g. [FCOS](https://pytorch.org/vision/stable/models/generated/torchvision.models.detection.fcos_resnet50_fpn.html#torchvision.models.detection.fcos_resnet50_fpn)), the behavior changes if in `train` or `eval` mode, but we don't do exactly what they do. In `train` mode, we return the *unprocessed* scores/classes/bboxes grids. In `eval` mode, we return the *processed* classes/bboxes in the form of a list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab50511d",
   "metadata": {},
   "source": [
    "![](model.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3262d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.vgg16(weights='DEFAULT').features\n",
    "        self.scores = torch.nn.Conv2d(512, 1, 1)\n",
    "        self.classes = torch.nn.Conv2d(512, 20, 1)\n",
    "        self.bboxes = torch.nn.Conv2d(512, 4, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        scores = self.scores(x)\n",
    "        classes = self.classes(x)\n",
    "        bboxes = self.bboxes(x)\n",
    "        if not self.training:\n",
    "            # when in evaluation mode, convert the output grid into a list of bboxes/classes\n",
    "            scores = torch.sigmoid(scores)\n",
    "            hasobjs = scores >= 0.5\n",
    "            scores = inv_scores(hasobjs, scores)\n",
    "            bboxes = od.grid.inv_offset_logsize_bboxes(hasobjs, bboxes)\n",
    "            classes = od.grid.inv_classes(hasobjs, classes)\n",
    "            bboxes, classes = od.post.NMS(probs, bboxes, classes)\n",
    "            return bboxes, classes\n",
    "        return scores, bboxes, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b6817",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91b1bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel().to(device)\n",
    "scores_loss = torch.nn.BCEWithLogitsLoss()\n",
    "bboxes_loss = torch.nn.MSELoss(reduction='none')\n",
    "classes_loss = torch.nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d1ab2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    avg_loss = 0\n",
    "    for imgs, targets in tqdm(tr, leave=False):\n",
    "        imgs = imgs.to(device)\n",
    "        preds_scores, preds_bboxes, preds_classes = model(imgs)\n",
    "\n",
    "        slices = od.grid.slices_center_locations(8, 8, targets['bboxes'])\n",
    "        scores = od.grid.scores(8, 8, slices).to(device)\n",
    "        bboxes = od.grid.offset_logsize_bboxes(8, 8, slices, targets['bboxes']).to(device)\n",
    "        classes = od.grid.classes(8, 8, slices, targets['classes']).to(device)\n",
    "\n",
    "        loss_value = \\\n",
    "            scores_loss(preds_scores, scores) + \\\n",
    "            (scores * bboxes_loss(preds_bboxes, bboxes)).mean() + \\\n",
    "            (scores * classes_loss(preds_classes, classes)).mean()\n",
    "        optimizer.zero_grad()\n",
    "        loss_value.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += float(loss_value) / len(tr)\n",
    "    print(f'Epoch {epoch+1}/{epochs} - Avg loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac761d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model(ds[0][0][None].to(device))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
