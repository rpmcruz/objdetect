<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>metrics API documentation</title>
<meta name="description" content="Implementation of Precision-Recall and AP metrics. This module is not yet fully tested, we recommend using torchmetrics." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>metrics</code></h1>
</header>
<section id="section-intro">
<p>Implementation of Precision-Recall and AP metrics. This module is not yet fully tested, we recommend using torchmetrics.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Implementation of Precision-Recall and AP metrics. This module is not yet fully tested, we recommend using torchmetrics.
&#39;&#39;&#39;

import torch

def IoU(bbox1, bbox2):
    &#39;&#39;&#39; Intersection over union between two bounding boxes. &#39;&#39;&#39;
    x0 = torch.maximum(bbox1[0], bbox2[0])
    y0 = torch.maximum(bbox1[1], bbox2[1])
    x1 = torch.minimum(bbox1[2], bbox2[2])
    y1 = torch.minimum(bbox1[3], bbox2[3])
    A1 = (bbox1[2]-bbox1[0]) * (bbox1[3]-bbox1[1])
    A2 = (bbox2[2]-bbox2[0]) * (bbox2[3]-bbox2[1])
    I = torch.clamp(x1-x0, min=0) * torch.clamp(y1-y0, min=0)
    U = A1 + A2 - I
    return I / U

def IoUs(bbox1, bboxes2):
    &#39;&#39;&#39; Intersection over union between one bounding box against a list of others. &#39;&#39;&#39;
    x0 = torch.maximum(bbox1[0], bboxes2[:, 0])
    y0 = torch.maximum(bbox1[1], bboxes2[:, 1])
    x1 = torch.minimum(bbox1[2], bboxes2[:, 2])
    y1 = torch.minimum(bbox1[3], bboxes2[:, 3])
    A1 = (bbox1[2]-bbox1[0]) * (bbox1[3]-bbox1[1])
    A2 = (bboxes2[:, 2]-bboxes2[:, 0]) * (bboxes2[:, 3]-bboxes2[:, 1])
    I = torch.clamp(x1-x0, min=0) * torch.clamp(y1-y0, min=0)
    U = A1 + A2 - I
    return I / U

def which_correct(preds, true, iou_threshold):
    &#39;&#39;&#39; For each bounding box in all image, computes if it was correctly predicted (that is, IoU is over the given threshold). For each true bounding box, it returns a boolean list of the same size indicating whether there is a matching prediction or not. &#39;&#39;&#39;
    return [
        [len(t[&#39;bboxes&#39;]) &gt; 0 and torch.any(IoUs(bb, t[&#39;bboxes&#39;]) &gt;= iou_threshold)
            for bb in p[&#39;bboxes&#39;]]
        for p, t in zip(preds, true)
    ]

def precision_recall_curve(preds, true, iou_threshold):
    &#39;&#39;&#39; Produces a precision-recall curve, given the has-object probabilities and respective bounding boxes. `preds` and `true` are lists of dictionaries containing at least: scores and bboxes. A good explanation of this metric: https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173. &#39;&#39;&#39;
    assert len(preds) == len(true), f&#39;number of preds ({len(preds)}) must match number of ground-truth ({len(true)})&#39;
    # match bounding by whether they are correct
    correct = which_correct(preds, true, iou_threshold)
    # flatten
    scores = torch.tensor([s for p in preds for s in p[&#39;scores&#39;]])
    correct = torch.tensor([c for cs in correct for c in cs])
    # order &#39;correct&#39; based on confidence probability
    ix = torch.argsort(scores, descending=True)
    correct = correct[ix]
    # compute curve
    npredictions = sum(len(p[&#39;bboxes&#39;]) for p in preds)
    cum_correct = torch.cumsum(correct, 0)
    precision = cum_correct / torch.arange(1, len(correct)+1)
    recall = cum_correct / npredictions
    # smooth zigzag
    for i in range(len(precision)-1, 0, -1):
        if precision[i-1] &lt; precision[i]:
            precision[i-1] = precision[i]
    precision = torch.cat((torch.tensor([1]), precision, torch.tensor([0])))
    recall = torch.cat((torch.tensor([0]), recall, torch.tensor([1])))
    return precision, recall

def AP(preds, true, iou_threshold):
    &#39;&#39;&#39; Produces an AP-score based on the precision-recall curve. &#39;&#39;&#39;
    precision, recall = precision_recall_curve(preds, true, iou_threshold)
    return torch.sum(torch.diff(recall) * precision[1:])

def filter_class(klass, preds, true, keys):
    &#39;&#39;&#39; Utility to filter a given class. &#39;&#39;&#39;
    preds = [{k: p[k][p[&#39;classes&#39;] == klass] for k in keys} for p in preds]
    true = [{k: t[k][t[&#39;classes&#39;] == klass] for k in keys} for t in true]
    return preds, true

def mAP(preds, true, iou_threshold):
    &#39;&#39;&#39; mAP = average AP for all classes. &#39;&#39;&#39;
    assert &#39;classes&#39; in preds[0] and &#39;classes&#39; in true[0]
    nclasses = 1+max([max(t[&#39;classes&#39;]) if len(t[&#39;classes&#39;]) else 0 for t in true])
    return sum(AP(*filter_class(klass, preds, true, [&#39;scores&#39;, &#39;bboxes&#39;]), iou_threshold) for klass in range(nclasses)) / nclasses

def mAP_ious(preds, true, iou_thresholds=torch.arange(0.05, 1, 0.05)):
    &#39;&#39;&#39; mAP = average AP for all classes and for a list of IoU thresholds. This is the metric used by MS-COCO. By default, we evaluate IoU@[0.05,0.95,0.05]. &#39;&#39;&#39;
    return torch.mean([mAP(preds, true, th) for th in iou_thresholds])

if __name__ == &#39;__main__&#39;:  # DEBUG
    true_bbox = torch.tensor([0.4, 0, 0.6, 1])
    pred_bboxes = torch.tensor([
        [0.0, 0, 0.3, 1],  # no intersection
        [0.5, 0, 0.6, 1],  # half intersection
        [0.4, 0, 0.6, 1],  # full intersection
    ])
    print(&#39;IoUs:&#39;, IoUs(true_bbox, pred_bboxes))
    true = [{&#39;bboxes&#39;: true_bbox[None]}]
    preds = [{&#39;bboxes&#39;: pred_bboxes, &#39;scores&#39;: [0.5, 0.4, 0.8]}]
    for th in [0, 0.5, 1]:
        print(f&#39;which_correct {th}:&#39;, which_correct(preds, true, th))
    for th in [0, 0.5, 1]:
        print(f&#39;precision_recall_curve {th}:&#39;, precision_recall_curve(preds, true, th))
    for th in [0, 0.5, 1]:
        print(f&#39;AP {th}:&#39;, AP(preds, true, th))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="metrics.AP"><code class="name flex">
<span>def <span class="ident">AP</span></span>(<span>preds, true, iou_threshold)</span>
</code></dt>
<dd>
<div class="desc"><p>Produces an AP-score based on the precision-recall curve.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def AP(preds, true, iou_threshold):
    &#39;&#39;&#39; Produces an AP-score based on the precision-recall curve. &#39;&#39;&#39;
    precision, recall = precision_recall_curve(preds, true, iou_threshold)
    return torch.sum(torch.diff(recall) * precision[1:])</code></pre>
</details>
</dd>
<dt id="metrics.IoU"><code class="name flex">
<span>def <span class="ident">IoU</span></span>(<span>bbox1, bbox2)</span>
</code></dt>
<dd>
<div class="desc"><p>Intersection over union between two bounding boxes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def IoU(bbox1, bbox2):
    &#39;&#39;&#39; Intersection over union between two bounding boxes. &#39;&#39;&#39;
    x0 = torch.maximum(bbox1[0], bbox2[0])
    y0 = torch.maximum(bbox1[1], bbox2[1])
    x1 = torch.minimum(bbox1[2], bbox2[2])
    y1 = torch.minimum(bbox1[3], bbox2[3])
    A1 = (bbox1[2]-bbox1[0]) * (bbox1[3]-bbox1[1])
    A2 = (bbox2[2]-bbox2[0]) * (bbox2[3]-bbox2[1])
    I = torch.clamp(x1-x0, min=0) * torch.clamp(y1-y0, min=0)
    U = A1 + A2 - I
    return I / U</code></pre>
</details>
</dd>
<dt id="metrics.IoUs"><code class="name flex">
<span>def <span class="ident">IoUs</span></span>(<span>bbox1, bboxes2)</span>
</code></dt>
<dd>
<div class="desc"><p>Intersection over union between one bounding box against a list of others.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def IoUs(bbox1, bboxes2):
    &#39;&#39;&#39; Intersection over union between one bounding box against a list of others. &#39;&#39;&#39;
    x0 = torch.maximum(bbox1[0], bboxes2[:, 0])
    y0 = torch.maximum(bbox1[1], bboxes2[:, 1])
    x1 = torch.minimum(bbox1[2], bboxes2[:, 2])
    y1 = torch.minimum(bbox1[3], bboxes2[:, 3])
    A1 = (bbox1[2]-bbox1[0]) * (bbox1[3]-bbox1[1])
    A2 = (bboxes2[:, 2]-bboxes2[:, 0]) * (bboxes2[:, 3]-bboxes2[:, 1])
    I = torch.clamp(x1-x0, min=0) * torch.clamp(y1-y0, min=0)
    U = A1 + A2 - I
    return I / U</code></pre>
</details>
</dd>
<dt id="metrics.filter_class"><code class="name flex">
<span>def <span class="ident">filter_class</span></span>(<span>klass, preds, true, keys)</span>
</code></dt>
<dd>
<div class="desc"><p>Utility to filter a given class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def filter_class(klass, preds, true, keys):
    &#39;&#39;&#39; Utility to filter a given class. &#39;&#39;&#39;
    preds = [{k: p[k][p[&#39;classes&#39;] == klass] for k in keys} for p in preds]
    true = [{k: t[k][t[&#39;classes&#39;] == klass] for k in keys} for t in true]
    return preds, true</code></pre>
</details>
</dd>
<dt id="metrics.mAP"><code class="name flex">
<span>def <span class="ident">mAP</span></span>(<span>preds, true, iou_threshold)</span>
</code></dt>
<dd>
<div class="desc"><p>mAP = average AP for all classes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mAP(preds, true, iou_threshold):
    &#39;&#39;&#39; mAP = average AP for all classes. &#39;&#39;&#39;
    assert &#39;classes&#39; in preds[0] and &#39;classes&#39; in true[0]
    nclasses = 1+max([max(t[&#39;classes&#39;]) if len(t[&#39;classes&#39;]) else 0 for t in true])
    return sum(AP(*filter_class(klass, preds, true, [&#39;scores&#39;, &#39;bboxes&#39;]), iou_threshold) for klass in range(nclasses)) / nclasses</code></pre>
</details>
</dd>
<dt id="metrics.mAP_ious"><code class="name flex">
<span>def <span class="ident">mAP_ious</span></span>(<span>preds, true, iou_thresholds=tensor([0.0500, 0.1000, 0.1500, 0.2000, 0.2500, 0.3000, 0.3500, 0.4000, 0.4500,
0.5000, 0.5500, 0.6000, 0.6500, 0.7000, 0.7500, 0.8000, 0.8500, 0.9000,
0.9500]))</span>
</code></dt>
<dd>
<div class="desc"><p>mAP = average AP for all classes and for a list of IoU thresholds. This is the metric used by MS-COCO. By default, we evaluate IoU@[0.05,0.95,0.05].</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mAP_ious(preds, true, iou_thresholds=torch.arange(0.05, 1, 0.05)):
    &#39;&#39;&#39; mAP = average AP for all classes and for a list of IoU thresholds. This is the metric used by MS-COCO. By default, we evaluate IoU@[0.05,0.95,0.05]. &#39;&#39;&#39;
    return torch.mean([mAP(preds, true, th) for th in iou_thresholds])</code></pre>
</details>
</dd>
<dt id="metrics.precision_recall_curve"><code class="name flex">
<span>def <span class="ident">precision_recall_curve</span></span>(<span>preds, true, iou_threshold)</span>
</code></dt>
<dd>
<div class="desc"><p>Produces a precision-recall curve, given the has-object probabilities and respective bounding boxes. <code>preds</code> and <code>true</code> are lists of dictionaries containing at least: scores and bboxes. A good explanation of this metric: <a href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173.">https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173.</a></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def precision_recall_curve(preds, true, iou_threshold):
    &#39;&#39;&#39; Produces a precision-recall curve, given the has-object probabilities and respective bounding boxes. `preds` and `true` are lists of dictionaries containing at least: scores and bboxes. A good explanation of this metric: https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173. &#39;&#39;&#39;
    assert len(preds) == len(true), f&#39;number of preds ({len(preds)}) must match number of ground-truth ({len(true)})&#39;
    # match bounding by whether they are correct
    correct = which_correct(preds, true, iou_threshold)
    # flatten
    scores = torch.tensor([s for p in preds for s in p[&#39;scores&#39;]])
    correct = torch.tensor([c for cs in correct for c in cs])
    # order &#39;correct&#39; based on confidence probability
    ix = torch.argsort(scores, descending=True)
    correct = correct[ix]
    # compute curve
    npredictions = sum(len(p[&#39;bboxes&#39;]) for p in preds)
    cum_correct = torch.cumsum(correct, 0)
    precision = cum_correct / torch.arange(1, len(correct)+1)
    recall = cum_correct / npredictions
    # smooth zigzag
    for i in range(len(precision)-1, 0, -1):
        if precision[i-1] &lt; precision[i]:
            precision[i-1] = precision[i]
    precision = torch.cat((torch.tensor([1]), precision, torch.tensor([0])))
    recall = torch.cat((torch.tensor([0]), recall, torch.tensor([1])))
    return precision, recall</code></pre>
</details>
</dd>
<dt id="metrics.which_correct"><code class="name flex">
<span>def <span class="ident">which_correct</span></span>(<span>preds, true, iou_threshold)</span>
</code></dt>
<dd>
<div class="desc"><p>For each bounding box in all image, computes if it was correctly predicted (that is, IoU is over the given threshold). For each true bounding box, it returns a boolean list of the same size indicating whether there is a matching prediction or not.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def which_correct(preds, true, iou_threshold):
    &#39;&#39;&#39; For each bounding box in all image, computes if it was correctly predicted (that is, IoU is over the given threshold). For each true bounding box, it returns a boolean list of the same size indicating whether there is a matching prediction or not. &#39;&#39;&#39;
    return [
        [len(t[&#39;bboxes&#39;]) &gt; 0 and torch.any(IoUs(bb, t[&#39;bboxes&#39;]) &gt;= iou_threshold)
            for bb in p[&#39;bboxes&#39;]]
        for p, t in zip(preds, true)
    ]</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="metrics.AP" href="#metrics.AP">AP</a></code></li>
<li><code><a title="metrics.IoU" href="#metrics.IoU">IoU</a></code></li>
<li><code><a title="metrics.IoUs" href="#metrics.IoUs">IoUs</a></code></li>
<li><code><a title="metrics.filter_class" href="#metrics.filter_class">filter_class</a></code></li>
<li><code><a title="metrics.mAP" href="#metrics.mAP">mAP</a></code></li>
<li><code><a title="metrics.mAP_ious" href="#metrics.mAP_ious">mAP_ious</a></code></li>
<li><code><a title="metrics.precision_recall_curve" href="#metrics.precision_recall_curve">precision_recall_curve</a></code></li>
<li><code><a title="metrics.which_correct" href="#metrics.which_correct">which_correct</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>